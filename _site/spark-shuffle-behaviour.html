<h4 id="background">Background</h4>

<p>The Spark has bottleneck on the shuffling while running jobs with non-trivial number of mappers and reducer. There has been lots of improvement in recent release on shuffling like consolidate file and sort-shuffling from version 1.1+.Here I have explained the <code class="highlighter-rouge">YARN</code> and  <code class="highlighter-rouge">Spark</code> parameter that are useful to optimize Spark shuffle performance.</p>

<h4 id="cluster-configuration">Cluster Configuration</h4>
<p>The cluster is Cloudera Enterprise Data Hub Edition Trial 5.3.1 with Spark 1.2.0 and Hadoop 2.5.0 .The following is the container configuration for  the cluster:</p>

<ul>
  <li><code class="highlighter-rouge">yarn.scheduler.minimum-allocation-mb = 1GB</code> , minimum container size</li>
  <li><code class="highlighter-rouge">yarn.scheduler.maximum-allocation-mb = 4GB</code> , maximum container size</li>
  <li><code class="highlighter-rouge">yarn.scheduler.minimum-allocation-vcores = 1</code>, minimum cores for container</li>
  <li><code class="highlighter-rouge">yarn.scheduler.maximum-allocation-vcores = 4,</code>, maximum cores for container</li>
  <li><code class="highlighter-rouge">yarn.nodemanager.resource.manager.memory-mb = 14GB</code>, amount of physical memory that can be allocated to containers</li>
  <li><code class="highlighter-rouge">yarn.nodemanager.resource.manager.cpu-vcores = 18 </code>, number of cores that can be allocated to containers</li>
</ul>

<p>From the above configuration, the spark executors memory size cannot be greater than 4GB  and number of cores assigned cannot be more than 4, else yarn ResourceManger could not start the executors.</p>

<p>Moreover, the cluster is using <a href="http://hadoop.apache.org/docs/r1.2.1/capacity_scheduler.html">capacity</a> scheduler so there is strict limit on user and group on the amount of cluster resources allocated. The cluster has following queue and resource allocation to each queue:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Queue Name</th>
      <th style="text-align: center">Percentage Resource Allocation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">dev</td>
      <td style="text-align: center">20%</td>
    </tr>
    <tr>
      <td style="text-align: center">rootuser</td>
      <td style="text-align: center">30%</td>
    </tr>
    <tr>
      <td style="text-align: center">other</td>
      <td style="text-align: center">40%</td>
    </tr>
  </tbody>
</table>

<p>The dev and rootuser queue have user-limit factor of 10, which allows the single user in the queue to use 10 times the configured capacity for the queue.</p>

<h4 id="spark-shuffle-behaviour">Spark Shuffle Behaviour</h4>

<p>The Terasort is well known benchamrk for Hadoop cluster which basic idea is to generate 1 TB random data , sort it (as fast as possible) and validate the sort data. We used similar Spark-Terasort for benachameking and tuning our cluster, which  which is available in <a href="https://github.com/ehiggs/spark-terasort">here</a> from <a href="https://github.com/ehiggs">Ewan Higgs</a>.Instead of 1 TB data we generate , sort and validate 100 GB data for our test purpose.Below is the command we use to generate the data:</p>

<p><code class="highlighter-rouge">spark-submit --class com.github.ehiggs.spark.terasort.TeraGen --deploy-mode client --master yarn --num-executors 26 --driver-memory 1g --executor-memory 1g --executor-cores 1 --queue dev /tmp/spark-terasort-1.0-SNAPSHOT-jar-with-dependencies.jar 100g /user/bijay/terasortInput</code></p>

<p>The configuration parameters for Spark shufle with default values are:
&gt; <code class="highlighter-rouge">spark.shuffle.consolidateFiles</code> 	<code class="highlighter-rouge">false</code>
&gt; <code class="highlighter-rouge">spark.shuffle.spill </code>	<code class="highlighter-rouge">true</code>
&gt; <code class="highlighter-rouge">spark.shuffle.spill.compress</code> 	<code class="highlighter-rouge">true</code>
&gt; <code class="highlighter-rouge">spark.shuffle.memoryFraction</code> 	<code class="highlighter-rouge">0.2</code>
&gt; <code class="highlighter-rouge">spark.shuffle.compress</code> 	<code class="highlighter-rouge">true</code>
&gt;<code class="highlighter-rouge">spark.shuffle.file.buffer.kb</code> 	<code class="highlighter-rouge">32</code>
&gt; <code class="highlighter-rouge">spark.reducer.maxMbInFlight</code> 	<code class="highlighter-rouge">48</code>
&gt;  <code class="highlighter-rouge">spark.shuffle.manager</code> 	<code class="highlighter-rouge">sort</code>
&gt; <code class="highlighter-rouge">spark.shuffle.sort.bypassMergeThreshold</code> 	<code class="highlighter-rouge">200</code>
&gt; <code class="highlighter-rouge">spark.shuffle.blockTransferService</code> 	<code class="highlighter-rouge">netty</code></p>

<p>Spark Sort:</p>

<p><code class="highlighter-rouge">spark-submit --class com.github.ehiggs.spark.terasort.TeraSort --deploy-mode client --master yarn --num-executors 30 --driver-memory 1g --executor-memory 1g --executor-cores 2 --queue dev /tmp/spark-terasort-1.0-SNAPSHOT-jar-with-dependencies.jar /user/bijay/terasortInput /user/bijay/terasortOutput</code></p>

<p>The resource usage for the sorting job with the <strong>default Spark shuffle configuration</strong> values:</p>

<blockquote>
  <p><strong>Details for Stage 1</strong></p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Parameters</th>
      <th>Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total task times across all task</td>
      <td>3.7 h</td>
    </tr>
    <tr>
      <td>Input</td>
      <td>93.1 GB</td>
    </tr>
    <tr>
      <td>Shuffle Write</td>
      <td>5.0 GB</td>
    </tr>
    <tr>
      <td>Shuffle Spill (memory )</td>
      <td>307.8 MB</td>
    </tr>
    <tr>
      <td>Shuffle Spill (disk)</td>
      <td>25.4 MB</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Details for stage 2</strong></p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Parameters</th>
      <th>Vlaue</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total task times across all task</td>
      <td>19.3 h</td>
    </tr>
    <tr>
      <td>Output</td>
      <td>93.1 GB</td>
    </tr>
    <tr>
      <td>Shuffle Read</td>
      <td>5.0 GB</td>
    </tr>
    <tr>
      <td>Shuffle spill (memory)</td>
      <td>135.4 GB</td>
    </tr>
    <tr>
      <td>Shuffle spill (disk)</td>
      <td>4.0 GB</td>
    </tr>
  </tbody>
</table>

<h4 id="terasort-after-changing-the-spark-shuffle-configuration">TeraSort after changing the Spark Shuffle Configuration</h4>

<p>Following changes are made to default Spark Shuffle Configuration:
&gt;<code class="highlighter-rouge">spark.shuffle.consolidateFiles=true</code>  ` create consolidates files during shuffle `
&gt;<code class="highlighter-rouge">spark.shuffle.memoryFraction=0.4</code> <code class="highlighter-rouge">Fraction of Java heap to use for aggregation and cogroups during shuffles is increased by 2 times</code>
&gt;<code class="highlighter-rouge">spark.shuffle.file.buffer.kb=64</code> <code class="highlighter-rouge">Size of the in-memory buffer for each shuffle file output stream, in kilobytes is increased by 2 times</code>
&gt;<code class="highlighter-rouge">spark.reducer.maxMbInFlight=96</code> <code class="highlighter-rouge">Maximum size (in megabytes) of map outputs to fetch simultaneously from each reduce task is increased by 2 times</code></p>

<p>With the above configuration change the metrics for the TeraSort job is shown below:</p>

<blockquote>
  <p><strong>Stage 1</strong></p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Parameters</th>
      <th>Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total task times across all task</td>
      <td>3.6 h</td>
    </tr>
    <tr>
      <td>Input</td>
      <td>93.1 GB</td>
    </tr>
    <tr>
      <td>Shuffle Write</td>
      <td>5.0 GB</td>
    </tr>
    <tr>
      <td>Shuffle Spill (memory )</td>
      <td>0 MB</td>
    </tr>
    <tr>
      <td>Shuffle Spill (disk)</td>
      <td>0 MB</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Stage 2</strong></p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Parameters</th>
      <th>Vlaue</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Total task times across all task</td>
      <td>14.5 h</td>
    </tr>
    <tr>
      <td>Output</td>
      <td>93.1 GB</td>
    </tr>
    <tr>
      <td>Shuffle Read</td>
      <td>4.8 GB</td>
    </tr>
    <tr>
      <td>Shuffle spill (memory)</td>
      <td>122.5 GB GB</td>
    </tr>
    <tr>
      <td>Shuffle spill (disk)</td>
      <td>3.4 GB</td>
    </tr>
  </tbody>
</table>

<p>These are the basic <code class="highlighter-rouge">Spark</code> and <code class="highlighter-rouge">YARN</code> parameters that can be used to tune to increse the <code class="highlighter-rouge">Spark</code> performance. There isnâ€™t any perfect configuration but it all depends on your jobs and workloads, data distribution and parallelizing of the workflow. You should try with different permuation before finding the one that fits your use case.</p>
