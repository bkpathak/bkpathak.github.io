<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bijay K Pathak</title>
    <description></description>
    <link>http://bkpathak.github.io</link>
    <atom:link
      href="http://bkpathak.github.io/atom.xml" rel="self"
      type="application/rss+xml" />
    
    <item>
      <title>Scala Substitution Model</title>
      <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Scala&lt;/code&gt; model of expression evaluation is based on the principle of substitution
model, in which variable names are replaced by the values they are bound to.The underlying
idea is that all evaluation does is reduce an expression to a value, which is a term that
does not need further evaluation. Values not include only constants, but tuple, constructors, and
functions. The substitution model is formalized in the $\lambda-calculus$, which gives foundation for functional programming.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;Scala&lt;/code&gt; program expressions are evaluated in the same way we would evaluate a mathematical expression. For example we know the expression $(2&lt;em&gt;2) + (3 * 4)$ will be evaluated by first evaluating $2 * 2$ and $3&lt;/em&gt;4$ and finally $4 + 12$. &lt;code class=&quot;highlighter-rouge&quot;&gt;Scala&lt;/code&gt; evaluation works in same way. Similarly in &lt;code class=&quot;highlighter-rouge&quot;&gt;Scala&lt;/code&gt; non-primitive expression is evaluated as follows.
1. Take the leftmost operator.
2. Evaluate its operators (left before right).
3. Apply the operator to operand values.&lt;/p&gt;

&lt;p&gt;During the process, &lt;code class=&quot;highlighter-rouge&quot;&gt;Scala&lt;/code&gt; evaluator &lt;em&gt;rewrites&lt;/em&gt; the program expression to another expression. The evaluation process finally converts the expression into value and the evaluation stops: assuming evaluation terminates else evaluation fails to reduce to final value and we have &lt;em&gt;infinite loop&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;expression-evaluation&quot;&gt;Expression Evaluation&lt;/h3&gt;
&lt;p&gt;Expression evaluation works by rewriting the original expression. Rewriting works by performing simple steps called &lt;em&gt;reductions&lt;/em&gt;. For example, the evaluation of arithmetic expression is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; def x = 2
scala&amp;gt; def y = 5
scala&amp;gt; (2 * x) + (4 * y)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;$\to (2 * 2) + (4 * y)$&lt;/p&gt;

&lt;p&gt;$\to 4 + (4 * 5)$&lt;/p&gt;

&lt;p&gt;$\to 4 + 20$&lt;/p&gt;

&lt;p&gt;$\to 24$&lt;/p&gt;

&lt;h3 id=&quot;function-evaluation&quot;&gt;Function Evaluation&lt;/h3&gt;
&lt;p&gt;Function with parameters are evaluated similar to the operators in expressions. Following are the rules for function parameters evaluation:
1. Evaluates all the function arguments from left-to-right.
2. Replace the function application by the functions right-hand side, and, at the same time
3. Replace all the formal parameters of the function by the actual arguments.&lt;/p&gt;

&lt;p&gt;Below is the evaluation of the function parameters:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; def square(x: Double) = x * x
scala&amp;gt; square(3 + 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;$\to square(6)$&lt;/p&gt;

&lt;p&gt;$\to 6 * 6$&lt;/p&gt;

&lt;p&gt;$\to 36$&lt;/p&gt;

&lt;h3 id=&quot;function-evaluation-strategy&quot;&gt;Function Evaluation Strategy&lt;/h3&gt;
&lt;p&gt;There are two evaluation strategy for function with parameters namely &lt;code class=&quot;highlighter-rouge&quot;&gt;call-by-value&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;call-by-name&lt;/code&gt;. For expressions that use only pure function and can be reduced with substitution model, both yields the same final result. Let’s define the function &lt;code class=&quot;highlighter-rouge&quot;&gt;sumofSqaures&lt;/code&gt; and evaluate it with both &lt;code class=&quot;highlighter-rouge&quot;&gt;call-by-value&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;call-by-name&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; def sumofSqaures(x: Int, y: Int) = square(x) + square(y)
scala&amp;gt; sumofSqaures(2, 2 + 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;call-by-value&lt;/em&gt;: &lt;em&gt;Call-by-value&lt;/em&gt; evaluates every function argument only once thus it avoids the repeated evaluation of arguments. Example:
$\to sumofSqaures(2, 5)$&lt;/p&gt;

    &lt;p&gt;$\to square(2) + square(5)$&lt;/p&gt;

    &lt;p&gt;$\to 2*2 + 5 * 5$&lt;/p&gt;

    &lt;p&gt;$\to 4 + 25$&lt;/p&gt;

    &lt;p&gt;$\to 29$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;call-by-name&lt;/em&gt;: &lt;em&gt;Call-by-name&lt;/em&gt; avoids evaluation of parameters if it is not used in the function body. Example:&lt;/p&gt;

    &lt;p&gt;$\to square(2) + sqaure(2 + 3)$&lt;/p&gt;

    &lt;p&gt;$\to square(2) + square( 2 + 3)$&lt;/p&gt;

    &lt;p&gt;$\to 2 * 2 + square(2 + 3)$&lt;/p&gt;

    &lt;p&gt;$\to 4 + (2 + 3) * (2 + 3)$&lt;/p&gt;

    &lt;p&gt;$\to 4 + 5 * (2 * 3)$&lt;/p&gt;

    &lt;p&gt;$\to 4 + 5 * 5$&lt;/p&gt;

    &lt;p&gt;$\to 4 + 25$&lt;/p&gt;

    &lt;p&gt;$\to 29$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;difference-between-call-by-value-and--call-by-name&quot;&gt;Difference between &lt;em&gt;Call-by-value&lt;/em&gt; and  &lt;em&gt;Call-by-name&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Call-by-value&lt;/em&gt; is more efficient then &lt;em&gt;call-by-name&lt;/em&gt; . If &lt;em&gt;call-by-value&lt;/em&gt; evaluation of expression terminates then &lt;em&gt;call-by-name&lt;/em&gt; evaluation also terminates, too but the other direction is not true since &lt;em&gt;Call-by-value&lt;/em&gt;  might loop but &lt;em&gt;call-by-name&lt;/em&gt; would terminate. Example:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
  scala&amp;gt; def loop:Int = loop
  scala&amp;gt; def test(x: Int, y: Int) = x
 &lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then the evaluation of function &lt;code class=&quot;highlighter-rouge&quot;&gt;test(1, loop)&lt;/code&gt; is:
  1. &lt;em&gt;Call-by-name&lt;/em&gt; evaluation reduces to $1$.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; $\to 1$
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Call-by-value&lt;/em&gt; evaluation leads to infinite loop.&lt;/p&gt;

    &lt;p&gt;$\to test(1, loop)$&lt;/p&gt;

    &lt;p&gt;$\to test(1, loop)$&lt;/p&gt;

    &lt;p&gt;$\to …$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Scala&lt;/code&gt; uses &lt;em&gt;call-by-value&lt;/em&gt; as a default since &lt;em&gt;call-by-value&lt;/em&gt; is often exponentially efficient then &lt;em&gt;call-by-name&lt;/em&gt;. We can force it to use &lt;em&gt;call-by-name&lt;/em&gt; by preceding the parameter types by $\Rightarrow$. Example:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scala&amp;gt; def constOne(x: Int, y: &lt;/code&gt;$\Rightarrow$&lt;code class=&quot;highlighter-rouge&quot;&gt; Int) = 1&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Following function call reduces to 1.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; constOne(1, loop)

unnamed0: Int = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And the below function call leads to infinite loop.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
scala&amp;gt; constOne(loop, 1) // Goes to infinite loop.
&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The actual operation of &lt;code class=&quot;highlighter-rouge&quot;&gt;Scala&lt;/code&gt; interpretor and compiler is more complex under the hood. Nevertheless, understanding the evaluation model helps us to know what the programming is doing and reason behind its correctness. The evaluation model is abstraction that hides the compiler details and help us understands the function parameters evaluations.&lt;/p&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/learn/progfun1/home/welcome&quot;&gt;Functional Programming Principles in Scala&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
      <pubDate>
        Sun, 12 Jun 2016 00:00:00 -0700
      </pubDate>
      <link>http://bkpathak.github.io/scala-substitution-model</link>
      <guid isPermaLink="true">http://bkpathak.github.io/scala-substitution-model</guid>
    </item>
    
    <item>
      <title>Python and Vim</title>
      <description>&lt;p&gt;Now days I am spending a quite a bit of time with Python from doing hobby project to implementing algorithms and data structures to writing Apache Spark code. Python is one of my favorite language. It’s versatile easy to grasp and the logic behind the code can be expressed with simplicity and elegance. The Zen of Python from Tim Peters says all about Python. Zen of Python can be read in Python shell with the command &lt;code class=&quot;highlighter-rouge&quot;&gt;import this&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I wrote Python with &lt;a href=&quot;https://www.jetbrains.com/pycharm/&quot;&gt;PyCharm&lt;/a&gt; and with two current popular text editor &lt;a href=&quot;https://atom.io/&quot;&gt;Atom&lt;/a&gt; and &lt;a href=&quot;http://www.sublimetext.com/&quot;&gt;Sublime&lt;/a&gt;. PyCharm is best editor though sometime it feels slow. And both Atom and Sublime has good plugins that makes it simple and easier to work with Python.&lt;/p&gt;

&lt;p&gt;If you are a Vim user and comfortable with Vim basics Vim could be used as complete Python IDE and there are very good Vim plugins support which makes working with Pyhton in Vim make smooth. Below are the steps for getting started to work with Vim.&lt;/p&gt;

&lt;p&gt;Most of the &lt;code class=&quot;highlighter-rouge&quot;&gt;*nix&lt;/code&gt; systems comes with Vim pre-installed. To check the Vim version and the supported Python:
&lt;code class=&quot;highlighter-rouge&quot;&gt;
vim --version
&lt;/code&gt;
 The output from the above command should show:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Vim version should be at least &amp;gt; 7.3.
There should be +Python under feature included&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One important things to note is the Pyhton version. Vim in Ubuntu 14.04 doesn’t come up with Python3 and we cannot use both Python2 and Python3 in same Vim session. To use Vim with Python3, we have to compile it with Python3 support. &lt;a href=&quot;http://www.xorpd.net/blog/vim_python3_install.html&quot;&gt;Here&lt;/a&gt; is steps to compile Vim with Python support.&lt;/p&gt;

&lt;h3 id=&quot;vim-extension-manger&quot;&gt;Vim Extension Manger&lt;/h3&gt;

&lt;p&gt;There are several extension manager available in Vim but the &lt;a href=&quot;https://github.com/VundleVim&quot;&gt;Vundle&lt;/a&gt; is quite popular one. It makes updating and installing Vim extensions lot easier. Below are the instructions from Vundle github repo to setup Vundle:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now create the &lt;code class=&quot;highlighter-rouge&quot;&gt;.vimrc&lt;/code&gt; file under user home directory and add the following lines:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set nocompatible              &quot; required
filetype off                  &quot; required

&quot; set the runtime path to include Vundle and initialize
set rtp+=~/.vim/bundle/Vundle.vim
call vundle#begin()
&quot; alternatively, pass a path where Vundle should install plugins
&quot;call vundle#begin(&#39;~/some/path/here&#39;)

&quot; let Vundle manage Vundle, required
Plugin &#39;VundleVim/Vundle.vim&#39;

&quot; All of your Plugins must be added before the following line
call vundle#end()            &quot; required
filetype plugin indent on    &quot; required
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now the Vundle is configured and we can open the Vim and start the Vundle to do all the magic. Inside the Vim type the command:
&lt;code class=&quot;highlighter-rouge&quot;&gt;
:PluginInstall
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now we can start adding extensions to Vim:&lt;/p&gt;

&lt;h3 id=&quot;code-folding&quot;&gt;Code Folding&lt;/h3&gt;
&lt;p&gt;We can have the Code folding feature of most modern IDE in Vim. &lt;a href=&quot;https://github.com/tmhedberg/SimpylFold&quot;&gt;SimplyFold&lt;/a&gt; is one such Vim extension. Add following line to &lt;code class=&quot;highlighter-rouge&quot;&gt;.vimrc&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;
Plugin &#39;tmhedberg/SimpylFold&#39;
&lt;/code&gt;
And open the &lt;code class=&quot;highlighter-rouge&quot;&gt;vim&lt;/code&gt; and run &lt;code class=&quot;highlighter-rouge&quot;&gt;:PluginInstall&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;python-indentation&quot;&gt;Python Indentation&lt;/h3&gt;
&lt;p&gt;For the &lt;code class=&quot;highlighter-rouge&quot;&gt;PEP8&lt;/code&gt; indentation add the following line to the &lt;code class=&quot;highlighter-rouge&quot;&gt;.vimrc&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;au BufNewFile,BufRead *.py
    \ set tabstop=4
    \ set softtabstop=4
    \ set shiftwidth=4
    \ set textwidth=119
    \ set expandtab
    \ set autoindent
    \ set fileformat=unix
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;This will give us 4 spaces when hit tab and line length doesn’t cross 120 characters(80 char is standard but with wider display we can go little more) and stores in unix format to avoid conversion issues.&lt;/p&gt;

&lt;h3 id=&quot;file-browsing&quot;&gt;File Browsing&lt;/h3&gt;
&lt;p&gt;NerdTree(https://github.com/scrooloose/nerdtree) provides the tree like file browser. Again to add using vundle add following lines to &lt;code class=&quot;highlighter-rouge&quot;&gt;.vimrc&lt;/code&gt;:
&lt;code class=&quot;highlighter-rouge&quot;&gt;
Bundle &#39;scrooloose/nerdtree&#39;
&lt;/code&gt;
And open &lt;code class=&quot;highlighter-rouge&quot;&gt;vim&lt;/code&gt; run &lt;code class=&quot;highlighter-rouge&quot;&gt;:PluginInstall&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;python-mode&quot;&gt;Python Mode&lt;/h3&gt;
&lt;p&gt;Python-mode(https://github.com/klen/python-mode) adds lots of feature such as Lint, codecompletio, document lookup, jump to classes, refactoring tools.&lt;/p&gt;

&lt;p&gt;The bundle for &lt;code class=&quot;highlighter-rouge&quot;&gt;python-mode&lt;/code&gt; is:
&lt;code class=&quot;highlighter-rouge&quot;&gt;
Bundle &#39;klen/python-mode&#39;
&lt;/code&gt;
And run &lt;code class=&quot;highlighter-rouge&quot;&gt;:PluginInstall&lt;/code&gt; to install. The complete documentation is provided on github or from inside the vim run &lt;code class=&quot;highlighter-rouge&quot;&gt;:help python-mode&lt;/code&gt;. Below are the some settings that can be set for &lt;code class=&quot;highlighter-rouge&quot;&gt;python-mode&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;let g:pymode_rope = 1

&quot; Documentation
let g:pymode_doc = 1
let g:pymode_doc_key = &#39;K&#39;

&quot;Linting
let g:pymode_lint = 1
let g:pymode_lint_checker = &quot;pyflakes,pep8&quot;
&quot; Auto check on save
let g:pymode_lint_write = 1

&quot; Support virtualenv
let g:pymode_virtualenv = 1

&quot; Enable breakpoints plugin
let g:pymode_breakpoint = 1
let g:pymode_breakpoint_bind = &#39;&amp;lt;leader&amp;gt;b&#39;

&quot; syntax highlighting
let g:pymode_syntax = 1
let g:pymode_syntax_all = 1
let g:pymode_syntax_indent_errors = g:pymode_syntax_all
let g:pymode_syntax_space_errors = g:pymode_syntax_all

&quot; Don&#39;t autofold code
let g:pymode_folding = 0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The above settings helps:
- Lookup Python docs using &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt;
- Automatically check code on saving, but only use &lt;code class=&quot;highlighter-rouge&quot;&gt;PyFlint&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;PyFlakes&lt;/code&gt;
- Support virtualenv
- Use &lt;leader&gt;b to add a pdb shortcut (inserts import pdb; pdb.set_trace() ### XXX BREAKPOINT into your code
- Enhanced syntax highlighting and formatting&lt;/leader&gt;&lt;/p&gt;

&lt;p&gt;These are the basic settings that makes using with Python. I found these pointers in different blogs and sites and was very helpful to setup Vim for Python, so I posted here so someone may find it useful.&lt;/p&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;https://wiki.python.org/moin/Vim&lt;/li&gt;
  &lt;li&gt;https://realpython.com/blog/python/vim-and-python-a-match-made-in-heaven/&lt;/li&gt;
&lt;/ol&gt;
</description>
      <pubDate>
        Sun, 29 Nov 2015 00:00:00 -0800
      </pubDate>
      <link>http://bkpathak.github.io/python-and-vim</link>
      <guid isPermaLink="true">http://bkpathak.github.io/python-and-vim</guid>
    </item>
    
    <item>
      <title>Spark Shuffle Behaviour</title>
      <description>&lt;h4 id=&quot;background&quot;&gt;Background&lt;/h4&gt;

&lt;p&gt;The Spark has bottleneck on the shuffling while running jobs with non-trivial number of mappers and reducer. There has been lots of improvement in recent release on shuffling like consolidate file and sort-shuffling from version 1.1+.Here I have explained the &lt;code class=&quot;highlighter-rouge&quot;&gt;YARN&lt;/code&gt; and  &lt;code class=&quot;highlighter-rouge&quot;&gt;Spark&lt;/code&gt; parameter that are useful to optimize Spark shuffle performance.&lt;/p&gt;

&lt;h4 id=&quot;cluster-configuration&quot;&gt;Cluster Configuration&lt;/h4&gt;
&lt;p&gt;The cluster is Cloudera Enterprise Data Hub Edition Trial 5.3.1 with Spark 1.2.0 and Hadoop 2.5.0 .The following is the container configuration for  the cluster:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;yarn.scheduler.minimum-allocation-mb = 1GB&lt;/code&gt; , minimum container size&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;yarn.scheduler.maximum-allocation-mb = 4GB&lt;/code&gt; , maximum container size&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;yarn.scheduler.minimum-allocation-vcores = 1&lt;/code&gt;, minimum cores for container&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;yarn.scheduler.maximum-allocation-vcores = 4,&lt;/code&gt;, maximum cores for container&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;yarn.nodemanager.resource.manager.memory-mb = 14GB&lt;/code&gt;, amount of physical memory that can be allocated to containers&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;yarn.nodemanager.resource.manager.cpu-vcores = 18 &lt;/code&gt;, number of cores that can be allocated to containers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the above configuration, the spark executors memory size cannot be greater than 4GB  and number of cores assigned cannot be more than 4, else yarn ResourceManger could not start the executors.&lt;/p&gt;

&lt;p&gt;Moreover, the cluster is using &lt;a href=&quot;http://hadoop.apache.org/docs/r1.2.1/capacity_scheduler.html&quot;&gt;capacity&lt;/a&gt; scheduler so there is strict limit on user and group on the amount of cluster resources allocated. The cluster has following queue and resource allocation to each queue:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Queue Name&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Percentage Resource Allocation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;dev&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;rootuser&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;30%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;other&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;40%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The dev and rootuser queue have user-limit factor of 10, which allows the single user in the queue to use 10 times the configured capacity for the queue.&lt;/p&gt;

&lt;h4 id=&quot;spark-shuffle-behaviour&quot;&gt;Spark Shuffle Behaviour&lt;/h4&gt;

&lt;p&gt;The Terasort is well known benchamrk for Hadoop cluster which basic idea is to generate 1 TB random data , sort it (as fast as possible) and validate the sort data. We used similar Spark-Terasort for benachameking and tuning our cluster, which  which is available in &lt;a href=&quot;https://github.com/ehiggs/spark-terasort&quot;&gt;here&lt;/a&gt; from &lt;a href=&quot;https://github.com/ehiggs&quot;&gt;Ewan Higgs&lt;/a&gt;.Instead of 1 TB data we generate , sort and validate 100 GB data for our test purpose.Below is the command we use to generate the data:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark-submit --class com.github.ehiggs.spark.terasort.TeraGen --deploy-mode client --master yarn --num-executors 26 --driver-memory 1g --executor-memory 1g --executor-cores 1 --queue dev /tmp/spark-terasort-1.0-SNAPSHOT-jar-with-dependencies.jar 100g /user/bijay/terasortInput&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The configuration parameters for Spark shufle with default values are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.consolidateFiles&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;false&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.spill &lt;/code&gt;	&lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.spill.compress&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.memoryFraction&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;0.2&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.compress&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.file.buffer.kb&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;32&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.reducer.maxMbInFlight&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;48&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.manager&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;sort&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.sort.bypassMergeThreshold&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;200&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.blockTransferService&lt;/code&gt; 	&lt;code class=&quot;highlighter-rouge&quot;&gt;netty&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spark Sort:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark-submit --class com.github.ehiggs.spark.terasort.TeraSort --deploy-mode client --master yarn --num-executors 30 --driver-memory 1g --executor-memory 1g --executor-cores 2 --queue dev /tmp/spark-terasort-1.0-SNAPSHOT-jar-with-dependencies.jar /user/bijay/terasortInput /user/bijay/terasortOutput&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The resource usage for the sorting job with the &lt;strong&gt;default Spark shuffle configuration&lt;/strong&gt; values:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Details for Stage 1&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameters&lt;/th&gt;
      &lt;th&gt;Values&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Total task times across all task&lt;/td&gt;
      &lt;td&gt;3.7 h&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input&lt;/td&gt;
      &lt;td&gt;93.1 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle Write&lt;/td&gt;
      &lt;td&gt;5.0 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle Spill (memory )&lt;/td&gt;
      &lt;td&gt;307.8 MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle Spill (disk)&lt;/td&gt;
      &lt;td&gt;25.4 MB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Details for stage 2&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameters&lt;/th&gt;
      &lt;th&gt;Vlaue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Total task times across all task&lt;/td&gt;
      &lt;td&gt;19.3 h&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output&lt;/td&gt;
      &lt;td&gt;93.1 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle Read&lt;/td&gt;
      &lt;td&gt;5.0 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle spill (memory)&lt;/td&gt;
      &lt;td&gt;135.4 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle spill (disk)&lt;/td&gt;
      &lt;td&gt;4.0 GB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;terasort-after-changing-the-spark-shuffle-configuration&quot;&gt;TeraSort after changing the Spark Shuffle Configuration&lt;/h4&gt;

&lt;p&gt;Following changes are made to default Spark Shuffle Configuration:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.consolidateFiles=true&lt;/code&gt;  ` create consolidates files during shuffle `&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.memoryFraction=0.4&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Fraction of Java heap to use for aggregation and cogroups during shuffles is increased by 2 times&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.file.buffer.kb=64&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Size of the in-memory buffer for each shuffle file output stream, in kilobytes is increased by 2 times&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.reducer.maxMbInFlight=96&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Maximum size (in megabytes) of map outputs to fetch simultaneously from each reduce task is increased by 2 times&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With the above configuration change the metrics for the TeraSort job is shown below:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Stage 1&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameters&lt;/th&gt;
      &lt;th&gt;Values&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Total task times across all task&lt;/td&gt;
      &lt;td&gt;3.6 h&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input&lt;/td&gt;
      &lt;td&gt;93.1 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle Write&lt;/td&gt;
      &lt;td&gt;5.0 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle Spill (memory )&lt;/td&gt;
      &lt;td&gt;0 MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle Spill (disk)&lt;/td&gt;
      &lt;td&gt;0 MB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Stage 2&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameters&lt;/th&gt;
      &lt;th&gt;Vlaue&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Total task times across all task&lt;/td&gt;
      &lt;td&gt;14.5 h&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output&lt;/td&gt;
      &lt;td&gt;93.1 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle Read&lt;/td&gt;
      &lt;td&gt;4.8 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle spill (memory)&lt;/td&gt;
      &lt;td&gt;122.5 GB GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shuffle spill (disk)&lt;/td&gt;
      &lt;td&gt;3.4 GB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;These are the basic &lt;code class=&quot;highlighter-rouge&quot;&gt;Spark&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;YARN&lt;/code&gt; parameters that can be used to tune to increse the &lt;code class=&quot;highlighter-rouge&quot;&gt;Spark&lt;/code&gt; performance. There isn’t any perfect configuration but it all depends on your jobs and workloads, data distribution and parallelizing of the workflow. You should try with different permuation before finding the one that fits your use case.&lt;/p&gt;
</description>
      <pubDate>
        Sat, 16 May 2015 00:00:00 -0700
      </pubDate>
      <link>http://bkpathak.github.io/spark-shuffle-behaviour</link>
      <guid isPermaLink="true">http://bkpathak.github.io/spark-shuffle-behaviour</guid>
    </item>
    
    <item>
      <title>Human Activity Recognition</title>
      <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;This is the project for the course &lt;a href=&quot;https://class.coursera.org/predmachlearn-005&quot;&gt;Practical Machine Learning&lt;/a&gt; from the &lt;a href=&quot;https://www.coursera.org/&quot;&gt;Coursera&lt;/a&gt;.This projects uses the accelerometer measurement of 6 people over the time. The data contains the accelerometer measurement mesurement for different type of acctivities and label identifying the quality of the activity.The goal of the project is to create the prediction model to predict the label for the test data sets given.The project describes each steps taken to build the model and all the preprocessing done in data sets to reach the mode.The data for the project is taken from [1].&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://bkpathak.github.io/human_activity_recognition/&quot;&gt;Here&lt;/a&gt; is the detail analysis with output.&lt;/p&gt;

&lt;h3 id=&quot;data-preprocessing-and-preparation&quot;&gt;Data Preprocessing and Preparation&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;caret&lt;/code&gt; package is used for this project.Training and test data sets are read and test data is not used until the model is built.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;library(caret)
ptrain &amp;lt;- read.csv(&quot;data/pml-training.csv&quot;)
ptest &amp;lt;- read.csv(&quot;data/pml-testing.csv&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To estimate the &lt;em&gt;out-of-sample&lt;/em&gt; error, the training set &lt;code class=&quot;highlighter-rouge&quot;&gt;ptrain&lt;/code&gt;  is splitted into training and validation set: &lt;code class=&quot;highlighter-rouge&quot;&gt;ptrain1&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;validation&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;set.seed(1000)
inTrain &amp;lt;- createDataPartition(y = ptrain$classe, p = 0.7, list = FALSE)
ptrain1 &amp;lt;- ptrain[inTrain, ]
validation &amp;lt;- ptrain[-inTrain, ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will now anlyse the each features of the &lt;code class=&quot;highlighter-rouge&quot;&gt;ptrain1&lt;/code&gt; and remove feauters which do not contribute much to the final model.The features with almost &lt;em&gt;zero and NA values&lt;/em&gt; are not useful for building model.Also, the feature having almost &lt;em&gt;zero variance&lt;/em&gt; do not contribute.Other feature variable such as name do not contribute the model but make the prediction model more complex.Now we will analyze the &lt;code class=&quot;highlighter-rouge&quot;&gt;ptrain1&lt;/code&gt; and remove those features and we will apply same to the &lt;code class=&quot;highlighter-rouge&quot;&gt;validation&lt;/code&gt; set also.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# remove features with nearly with zero variance using
nzVar &amp;lt;-  nearZeroVar(ptrain1)
ptrain1 &amp;lt;- ptrain1[, -nzVar]

# Applying same to validation set
validation &amp;lt;- validation[, -nzVar]

# check for features with mostly NAs value and remove if any
mostlyNAs &amp;lt;- sapply(ptrain1 , function(x) mean(is.na(x))) &amp;gt; 0.95
ptrain1 &amp;lt;- ptrain1[, mostlyNAs == F]
validation &amp;lt;- validation[, mostlyNAs == F]

# remove variables which are not relevant for building the model. The first 5 feature varible * X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp* are not relevant.
ptrain1 &amp;lt;- ptrain1[, -(1:5)]
validation &amp;lt;- validation[, -(1:5)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;model-construction&quot;&gt;Model Construction&lt;/h3&gt;

&lt;p&gt;First, I planned to build the model using &lt;code class=&quot;highlighter-rouge&quot;&gt;neural network&lt;/code&gt;.Since the problem is supervised learning and neural nets are good at classification task.To make the model more robust I will use 3 - fold cross validation to build the model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Use 3-fold cross validation to build the model
controlPara &amp;lt;- trainControl(method = &quot;cv&quot;, number = 3, verboseIter = F,)

#build model using ptrain1
modelFit1 &amp;lt;- train(classe ~ ., data = ptrain1, method = &quot;nnet&quot;, trControl = controlPara,trace = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;model-evaluation&quot;&gt;Model Evaluation&lt;/h3&gt;
&lt;p&gt;The fitted model is used to predict tje label for our validation set and find the &lt;em&gt;out-of-sample&lt;/em&gt; error.Confusion matrix is used to compare the output from the our model to the actual labels of the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# uses modelFit1 to predict classe for our validation data sets
prediction &amp;lt;- predict(modelFit1, newdata = validation)

# Show the confusion matrix with other relevant information to evaluate our model
confusionMatrix(validation$classe, prediction)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The performance of this model is less than satisfactory with just 38 % accuracy.The matrix shows there are lots of misclassification. The nnet is feed forward model so the neural network with back propagation may give more accuracy then just feed forward model.&lt;/p&gt;

&lt;h3 id=&quot;second-model-construction&quot;&gt;Second Model Construction&lt;/h3&gt;

&lt;p&gt;Let’s try to build the second model with &lt;code class=&quot;highlighter-rouge&quot;&gt;Random Forest&lt;/code&gt; and evaluate the accuracy of model on the validation data sets.The model is build with 3-fold CV.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Use 3-fold cross validation to build the model
controlPara &amp;lt;- trainControl(method = &quot;cv&quot;, number = 3, verboseIter = F,)

#build model using ptrain1
modelFit2 &amp;lt;- train(classe ~. , data = ptrain1 ,method = &quot;rf&quot;, trControl = controlPara )
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;second-model-evaulation&quot;&gt;Second Model Evaulation&lt;/h3&gt;

&lt;p&gt;The fitted model is used to predict tje label for our validation set and find the &lt;em&gt;out-of-sample&lt;/em&gt; error.Confusion matrix is used to compare the output from the our model to the actual labels of the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# uses modelFit1 to predict classe for our validation data sets
prediction &amp;lt;- predict(modelFit2, newdata = validation)

# Show the confusion matrix with other relevant information to evaluate our model
confusionMatrix(validation$classe, prediction)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;modelFit2&lt;/code&gt; build with &lt;code class=&quot;highlighter-rouge&quot;&gt;Random Forest&lt;/code&gt; is far better then our previos model &lt;code class=&quot;highlighter-rouge&quot;&gt;modelFit1&lt;/code&gt;.The &lt;code class=&quot;highlighter-rouge&quot;&gt;out-of-sample&lt;/code&gt; error for the model is less 1 %. This is the excellent model with 99 % accuarcy. We will use this model as our final model and used this to predict the classes on our test data sets.&lt;/p&gt;

&lt;h3 id=&quot;final-model&quot;&gt;Final Model&lt;/h3&gt;

&lt;p&gt;Since we have divided our origan training data sets &lt;code class=&quot;highlighter-rouge&quot;&gt;ptrain&lt;/code&gt; into &lt;code class=&quot;highlighter-rouge&quot;&gt;ptrain1&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;validation&lt;/code&gt; for finding the &lt;em&gt;out-of-sample&lt;/em&gt; error.Now we will build the model using complete trianing data sets &lt;code class=&quot;highlighter-rouge&quot;&gt;ptrain&lt;/code&gt;.We also apply the same data pre-processing steps on both &lt;code class=&quot;highlighter-rouge&quot;&gt;ptrain&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ptest&lt;/code&gt; data sets.&lt;/p&gt;

&lt;h4 id=&quot;preprocessing-on-traing-and-test-data-sets&quot;&gt;Preprocessing on traing and test data sets&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# remove features with nearly with zero variance using
nzVar &amp;lt;-  nearZeroVar(ptrain)
ptrain &amp;lt;- ptrain[, -nzVar]

# Applying same to test data set
ptest &amp;lt;- ptest[, -nzVar]

# check for features with mostly NAs value and remove if any
mostlyNAs &amp;lt;- sapply(ptrain , function(x) mean(is.na(x))) &amp;gt; 0.95
ptrain &amp;lt;- ptrain[, mostlyNAs == F]
ptest &amp;lt;- ptest[, mostlyNAs == F]

# remove variables which are not relevant for building the model. The first 5 feature varible * X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp* are not relevant.
ptrain &amp;lt;- ptrain[, -(1:5)]
ptest &amp;lt;- ptest[, -(1:5)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;building-the-final-model&quot;&gt;Building the Final Model&lt;/h4&gt;
&lt;p&gt;Now we fit the model on the complete training data sets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# Use 3-fold cross validation to build the model
controlPara &amp;lt;- trainControl(method = &quot;cv&quot;, number = 3, verboseIter = F)

#build model using ptrain
modelFinal &amp;lt;- train(classe ~. , data = ptrain ,method = &quot;rf&quot;, trControl = controlPara )

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;prediction-on-test-data-set&quot;&gt;Prediction on Test Data Set&lt;/h3&gt;
&lt;p&gt;Now we will use our final model &lt;code class=&quot;highlighter-rouge&quot;&gt;modelFinal&lt;/code&gt; to predict the class of our test data set.We have applied the same data preprocessing methods to test data sets as to training data sets.&lt;/p&gt;

&lt;p&gt;The test data sets &lt;code class=&quot;highlighter-rouge&quot;&gt;ptest&lt;/code&gt; contains 20 observations. The prediction output for each observation is written to the separate file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{r}&quot;&gt;# predict the class of test data
prediction &amp;lt;- predict(modelFinal, newdata = ptest)

# create the character vector for prediction class
preds &amp;lt;- as.character(prediction)

# write prediction for each observation to the files
write_to_files &amp;lt;- function(x) {
    n &amp;lt;- length(x)
    for(i in 1:n) {
        filename &amp;lt;- paste0(&quot;output/problem_id_&quot;, i, &quot;.txt&quot;)
        write.table(x[i], file=filename, quote=F, row.names=F, col.names=F)
    }
}

# create prediction files to submit
write_to_files(preds)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
  &lt;li&gt;Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ‘13) . Stuttgart, Germany: ACM SIGCHI, 2013.&lt;/li&gt;
&lt;/ol&gt;
</description>
      <pubDate>
        Sun, 21 Sep 2014 00:00:00 -0700
      </pubDate>
      <link>http://bkpathak.github.io/human-activity-recognition</link>
      <guid isPermaLink="true">http://bkpathak.github.io/human-activity-recognition</guid>
    </item>
    
  </channel>
</rss>
